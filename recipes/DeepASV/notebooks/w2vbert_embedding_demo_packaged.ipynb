{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e85a2b",
   "metadata": {},
   "source": [
    "# W2V-BERT Embedding Demo (Packaged)\n",
    "\n",
    "This variant uses the published `w2vbert_speaker` package inside the dedicated virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66c725e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /Users/zb/NWG/w2v-BERT-2.0_SV/recipes/DeepASV/notebooks\n",
      "Resolved repository root: /Users/zb/NWG/w2v-BERT-2.0_SV\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"recipes\").exists() and (candidate / \"deeplab\").exists():\n",
    "            return candidate\n",
    "    raise RuntimeError(f\"Unable to locate the repository root from {start}.\")\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "REPO_ROOT = find_repo_root(NOTEBOOK_DIR)\n",
    "\n",
    "print(f\"Notebook directory: {NOTEBOOK_DIR}\")\n",
    "print(f\"Resolved repository root: {REPO_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93e74052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: /Users/zb/NWG/pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0/model_lmft_0.14.pth\n",
      "Using local encoder weights from: /Users/zb/NWG/pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0\n",
      "Using device: cpu, target sample rate: 16000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from w2vbert_speaker import W2VBERT_SPK_Module\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def resolve_checkpoint(repo_root: Path) -> Path:\n",
    "    candidate_relatives = [\n",
    "        \"deeplab/pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0/model_lmft_0.14.pth\",\n",
    "        \"../pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0/model_lmft_0.14.pth\",\n",
    "        \"model_lmft_0.14.pth\",\n",
    "    ]\n",
    "    for relative in candidate_relatives:\n",
    "        candidate = (repo_root / relative).resolve()\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"Checkpoint model_lmft_0.14.pth was not found in known locations.\")\n",
    "\n",
    "def resolve_model_dir(repo_root: Path) -> Path:\n",
    "    candidate_relatives = [\n",
    "        \"deeplab/pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0\",\n",
    "        \"../pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0\",\n",
    "    ]\n",
    "    for relative in candidate_relatives:\n",
    "        candidate = (repo_root / relative).resolve()\n",
    "        if candidate.is_dir() and (candidate / \"model.safetensors\").exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"Local Transformer weights not found. Expected model.safetensors under a known directory.\")\n",
    "\n",
    "checkpoint_path = resolve_checkpoint(REPO_ROOT)\n",
    "model_dir = resolve_model_dir(REPO_ROOT)\n",
    "\n",
    "print(f\"Using checkpoint: {checkpoint_path}\")\n",
    "print(f\"Using local encoder weights from: {model_dir}\")\n",
    "\n",
    "embedding_model = W2VBERT_SPK_Module(device=DEVICE, model_path=str(model_dir)).load_model(checkpoint_path)\n",
    "spk_model = embedding_model.modules_dict[\"spk_model\"]\n",
    "target_sr = spk_model.front.feature_extractor.sampling_rate\n",
    "print(f\"Using device: {DEVICE}, target sample rate: {target_sr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73c7c0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using audio file: /Users/zb/NWG/datasets/voxceleb1test/wav/id10270/5r0dWxy17C8/00001.wav\n",
      "Loaded waveform shape: torch.Size([1, 133761]), sample rate: 16000\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "import torchaudio\n",
    "\n",
    "def resolve_audio_path(repo_root: Path) -> Path:\n",
    "    candidate_relatives = [\n",
    "        \"../datasets/voxceleb1test/wav/id10270/5r0dWxy17C8/00001.wav\",\n",
    "        \"datasets/voxceleb1test/wav/id10270/5r0dWxy17C8/00001.wav\",\n",
    "    ]\n",
    "    for relative in candidate_relatives:\n",
    "        candidate = (repo_root / relative).resolve()\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"Audio file not found. Update resolve_audio_path with a valid sample.\")\n",
    "\n",
    "def load_waveform(path: Path, target_sr: int) -> tuple[torch.Tensor, int]:\n",
    "    signal, sr = sf.read(str(path), dtype=\"float32\")\n",
    "    if signal.ndim > 1:\n",
    "        signal = signal.mean(axis=1)\n",
    "    waveform = torch.from_numpy(signal)\n",
    "    if sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "        waveform = resampler(waveform.unsqueeze(0)).squeeze(0)\n",
    "        sr = target_sr\n",
    "    waveform = waveform.unsqueeze(0).to(torch.float32)\n",
    "    return waveform, sr\n",
    "\n",
    "AUDIO_PATH = resolve_audio_path(REPO_ROOT)\n",
    "waveform, sr = load_waveform(AUDIO_PATH, target_sr)\n",
    "print(f\"Using audio file: {AUDIO_PATH}\")\n",
    "print(f\"Loaded waveform shape: {waveform.shape}, sample rate: {sr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf127f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (256,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.16015907, -0.8298738 ,  0.70560724, -0.14280552,  0.35778913,\n",
       "        0.47867072, -0.12521656, -0.90179497], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    embeddings = embedding_model(waveform.to(DEVICE))\n",
    "embedding_vector = embeddings.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(f\"Embedding shape: {embedding_vector.shape}\")\n",
    "embedding_vector[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c90e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "\n",
    "artifact_waveform = (REPO_ROOT / \"packages/w2vbert_speaker/artifacts/w2vbert_speaker_script.pt\").resolve()\n",
    "artifact_preprocessed = (REPO_ROOT / \"packages/w2vbert_speaker/artifacts/w2vbert_speaker_script_preprocessed.pt\").resolve()\n",
    "\n",
    "metadata_waveform = {\"sample_rate\": \"\", \"embedding_dim\": \"\"}\n",
    "scripted_waveform = torch.jit.load(str(artifact_waveform), map_location=\"cpu\", _extra_files=metadata_waveform)\n",
    "metadata_preprocessed = {\"sample_rate\": \"\", \"embedding_dim\": \"\", \"preprocessed\": \"\"}\n",
    "scripted_preprocessed = torch.jit.load(str(artifact_preprocessed), map_location=\"cpu\", _extra_files=metadata_preprocessed)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    waveform_scripted = waveform.cpu()\n",
    "    emb_waveform = scripted_waveform(waveform_scripted).squeeze(0).cpu()\n",
    "\n",
    "features = spk_model.front.feature_extractor(\n",
    "    waveform_scripted.squeeze(0).cpu().numpy(),\n",
    "    sampling_rate=target_sr,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=False,\n",
    "    truncation=False,\n",
    "    return_attention_mask=False,\n",
    ")\n",
    "input_features = features[\"input_features\"].to(torch.float32)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    emb_preprocessed = scripted_preprocessed(input_features).squeeze(0).cpu()\n",
    "\n",
    "cos_waveform = F.cosine_similarity(torch.from_numpy(embedding_vector).unsqueeze(0), emb_waveform.unsqueeze(0)).item()\n",
    "l2_waveform = torch.norm(torch.from_numpy(embedding_vector) - emb_waveform).item()\n",
    "cos_preprocessed = F.cosine_similarity(torch.from_numpy(embedding_vector).unsqueeze(0), emb_preprocessed.unsqueeze(0)).item()\n",
    "l2_preprocessed = torch.norm(torch.from_numpy(embedding_vector) - emb_preprocessed).item()\n",
    "\n",
    "print(f\"cosine(eager, waveform_script) = {cos_waveform:.6f}\")\n",
    "print(f\"    L2(eager, waveform_script) = {l2_waveform:.6e}\")\n",
    "print(f\"cosine(eager, preprocessed_script) = {cos_preprocessed:.6f}\")\n",
    "print(f\"    L2(eager, preprocessed_script) = {l2_preprocessed:.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1600a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-5\n",
    "if l2_preprocessed > tolerance:\n",
    "    raise AssertionError(f\"Preprocessed TorchScript deviates from eager more than {tolerance}: L2={l2_preprocessed}\")\n",
    "if l2_waveform > 1e-3:\n",
    "    print(f\"Warning: waveform TorchScript deviates from eager (L2={l2_waveform:.6e}). Use the preprocessed artifact for parity.\")\n",
    "else:\n",
    "    print(\"Waveform TorchScript within tolerance of eager model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c385194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-w2vbert-notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
