{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3ef101",
   "metadata": {},
   "source": [
    "# W2V-BERT TorchScript Demo\n",
    "\n",
    "Run `scripts/export_w2vbert_torchscript.py` first to generate the TorchScript artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50fa0ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /Users/zb/NWG/w2v-BERT-2.0_SV/recipes/DeepASV/notebooks\n",
      "Repository root: /Users/zb/NWG/w2v-BERT-2.0_SV\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"recipes\").exists() and (candidate / \"deeplab\").exists():\n",
    "            return candidate\n",
    "    raise RuntimeError(\"Unable to locate the repository root.\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(NOTEBOOK_DIR)\n",
    "print(f\"Notebook directory: {NOTEBOOK_DIR}\")\n",
    "print(f\"Repository root: {REPO_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b646ab4b",
   "metadata": {},
   "source": [
    "Prepare a clean environment with the minimal dependencies:\n",
    "\n",
    "```bash\n",
    "pip install torch soundfile librosa numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86570928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed artifact: /Users/zb/NWG/w2v-BERT-2.0_SV/packages/w2vbert_speaker/artifacts/w2vbert_speaker_script_preprocessed.pt\n",
      "Checkpoint: /Users/zb/NWG/pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0/model_lmft_0.14.pth\n",
      "Model directory: /Users/zb/NWG/pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0\n",
      "Loaded waveform TorchScript (embedding_dim=256, sample_rate=16000)\n",
      "Loaded preprocessed TorchScript (preprocessed flag=b'true')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Iterable\n",
    "\n",
    "def pick_first(paths: Iterable[Path]) -> Path:\n",
    "    for candidate in paths:\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    raise FileNotFoundError(\"None of the provided paths exist:\\n\" + \"\\n\".join(str(p) for p in paths))\n",
    "\n",
    "# ARTIFACT_WAVEFORM = pick_first([\n",
    "#     REPO_ROOT / \"packages/w2vbert_speaker/artifacts/w2vbert_speaker_script.pt\",\n",
    "# ])\n",
    "\n",
    "ARTIFACT_PREPROCESSED = pick_first([\n",
    "    REPO_ROOT / \"packages/w2vbert_speaker/artifacts/w2vbert_speaker_script_preprocessed.pt\",\n",
    "])\n",
    "\n",
    "CHECKPOINT_PATH = pick_first([\n",
    "    REPO_ROOT.parent / \"pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0/model_lmft_0.14.pth\",\n",
    "    REPO_ROOT / \"deeplab/pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0/model_lmft_0.14.pth\",\n",
    "])\n",
    "\n",
    "MODEL_DIR = pick_first([\n",
    "    REPO_ROOT.parent / \"pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0\",\n",
    "    REPO_ROOT / \"deeplab/pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0\",\n",
    "])\n",
    "\n",
    "# print(f\"Waveform artifact: {ARTIFACT_WAVEFORM}\")\n",
    "print(f\"Preprocessed artifact: {ARTIFACT_PREPROCESSED}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "\n",
    "metadata_waveform = {\"sample_rate\": \"\", \"embedding_dim\": \"\"}\n",
    "# scripted_waveform = torch.jit.load(str(ARTIFACT_WAVEFORM), map_location=\"cpu\", _extra_files=metadata_waveform)\n",
    "\n",
    "metadata_preprocessed = {\"sample_rate\": \"\", \"embedding_dim\": \"\", \"preprocessed\": \"\"}\n",
    "scripted_preprocessed = torch.jit.load(str(ARTIFACT_PREPROCESSED), map_location=\"cpu\", _extra_files=metadata_preprocessed)\n",
    "\n",
    "sample_rate = int(metadata_waveform[\"sample_rate\"] or metadata_preprocessed[\"sample_rate\"] or 16000)\n",
    "embedding_dim = int(metadata_waveform[\"embedding_dim\"] or metadata_preprocessed[\"embedding_dim\"] or -1)\n",
    "\n",
    "print(f\"Loaded waveform TorchScript (embedding_dim={embedding_dim}, sample_rate={sample_rate})\")\n",
    "print(f\"Loaded preprocessed TorchScript (preprocessed flag={metadata_preprocessed['preprocessed']!r})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5639b72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zb/NWG/w2v-BERT-2.0_SV/.venv_w2vbert_notebook/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu, target sample rate: 16000\n"
     ]
    }
   ],
   "source": [
    "from w2vbert_speaker import W2VBERT_SPK_Module\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "eager_model = W2VBERT_SPK_Module(device=DEVICE, model_path=str(MODEL_DIR)).load_model(CHECKPOINT_PATH)\n",
    "spk_module = eager_model.modules_dict[\"spk_model\"]\n",
    "feature_extractor = spk_module.front.feature_extractor\n",
    "target_sr = int(feature_extractor.sampling_rate)\n",
    "print(f\"Using device: {DEVICE}, target sample rate: {target_sr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1e677b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /Users/zb/NWG/datasets/voxceleb1test/wav/id10270/5r0dWxy17C8/00001.wav at sample rate 16000 with waveform shape torch.Size([1, 133761])\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "audio_candidates = [\n",
    "    REPO_ROOT.parent / \"datasets/voxceleb1test/wav/id10270/5r0dWxy17C8/00001.wav\",\n",
    "    REPO_ROOT / \"datasets/voxceleb1test/wav/id10270/5r0dWxy17C8/00001.wav\",\n",
    "]\n",
    "\n",
    "for candidate in audio_candidates:\n",
    "    if candidate.exists():\n",
    "        target_audio = candidate.resolve()\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"Audio file not found. Update audio_candidates with a valid sample path.\")\n",
    "\n",
    "signal, sr = sf.read(str(target_audio), dtype=\"float32\")\n",
    "if signal.ndim > 1:\n",
    "    signal = signal.mean(axis=1)\n",
    "if sr != target_sr:\n",
    "    signal = librosa.resample(signal, orig_sr=sr, target_sr=target_sr)\n",
    "    sr = target_sr\n",
    "\n",
    "waveform = torch.from_numpy(signal).unsqueeze(0).to(torch.float32)\n",
    "print(f\"Loaded {target_audio} at sample rate {sr} with waveform shape {waveform.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd1586ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager embedding preview: -0.1602 -0.8299 0.7056 -0.1428 0.3578 0.4787 -0.1252 -0.9018\n",
      "Preprocessed TorchScript preview: -0.1602 -0.8299 0.7056 -0.1428 0.3578 0.4787 -0.1252 -0.9018\n",
      "\n",
      "cosine(eager, preprocessed_script) = 1.000000\n",
      "    L2(eager, preprocessed_script) = 0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "with torch.inference_mode():\n",
    "    eager_embedding = eager_model(waveform.to(DEVICE)).squeeze(0).cpu()\n",
    "# with torch.inference_mode():\n",
    "#     scripted_embedding = scripted_waveform(waveform).squeeze(0).cpu()\n",
    "\n",
    "features = feature_extractor(\n",
    "    waveform.squeeze(0).cpu().numpy(),\n",
    "    sampling_rate=target_sr,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=False,\n",
    "    truncation=False,\n",
    "    return_attention_mask=False,\n",
    " )\n",
    "input_features = features[\"input_features\"].to(torch.float32)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    preprocessed_embedding = scripted_preprocessed(input_features).squeeze(0).cpu()\n",
    "\n",
    "# cos_waveform = F.cosine_similarity(eager_embedding.unsqueeze(0), scripted_embedding.unsqueeze(0)).item()\n",
    "# l2_waveform = torch.norm(eager_embedding - scripted_embedding).item()\n",
    "cos_preprocessed = F.cosine_similarity(eager_embedding.unsqueeze(0), preprocessed_embedding.unsqueeze(0)).item()\n",
    "l2_preprocessed = torch.norm(eager_embedding - preprocessed_embedding).item()\n",
    "\n",
    "def preview(vec: torch.Tensor, n: int = 8) -> str:\n",
    "    return \" \".join(f\"{value:.4f}\" for value in vec[:n].tolist())\n",
    "\n",
    "print(\"Eager embedding preview:\", preview(eager_embedding))\n",
    "# print(\"Waveform TorchScript preview:\", preview(scripted_embedding))\n",
    "print(\"Preprocessed TorchScript preview:\", preview(preprocessed_embedding))\n",
    "print()\n",
    "# print(f\"cosine(eager, waveform_script) = {cos_waveform:.6f}\")\n",
    "# print(f\"    L2(eager, waveform_script) = {l2_waveform:.6e}\")\n",
    "print(f\"cosine(eager, preprocessed_script) = {cos_preprocessed:.6f}\")\n",
    "print(f\"    L2(eager, preprocessed_script) = {l2_preprocessed:.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882508cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-5\n",
    "if l2_preprocessed > tolerance:\n",
    "    raise AssertionError(f\"Preprocessed TorchScript deviates from eager more than {tolerance}: L2={l2_preprocessed}\")\n",
    "# if l2_waveform > 1e-3:\n",
    "#     print(f\"Warning: waveform TorchScript deviates from eager (L2={l2_waveform:.6e}). Use the preprocessed artifact for parity.\")\n",
    "# else:\n",
    "#     print(\"Waveform TorchScript within tolerance of eager model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04fea74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "W2V-BERT Notebook (.venv_w2vbert_notebook)",
   "language": "python",
   "name": "w2vbert_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
