{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc0bfc2",
   "metadata": {},
   "source": [
    "# W2V-BERT Parity Demo: Packaged Eager vs Scripted Preprocessed\n",
    "\n",
    "This notebook demonstrates how to install the packaged eager model and the lightweight scripted runtime, how to load them from the notebook virtual environment, and how to verify numeric parity between the eager embedding and the preprocessed TorchScript artifact.\n",
    "Notes:\n",
    "- The notebook will attempt to install the local packages into the kernel's Python using `sys.executable -m pip install -e ...` if they are not already installed.\n",
    "- The scripted runtime is expected to use the saved Hugging Face feature_extractor to compute deterministic `input_features` that match the exporter.\n",
    "- This demo assumes the repository root contains `packages/w2vbert_speaker` and `packages/w2vbert_speaker_scripted`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0a3102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /Users/zb/NWG/w2v-BERT-2.0_SV/recipes/DeepASV/notebooks\n",
      "Resolved repository root: /Users/zb/NWG/w2v-BERT-2.0_SV\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: locate repository root and set up common helpers\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / 'recipes').exists() and (candidate / 'deeplab').exists():\n",
    "            return candidate\n",
    "    raise RuntimeError(f'Unable to locate the repository root from {start}.')\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "REPO_ROOT = find_repo_root(NOTEBOOK_DIR)\n",
    "print(f'Notebook directory: {NOTEBOOK_DIR}')\n",
    "print(f'Resolved repository root: {REPO_ROOT}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e69930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zb/NWG/w2v-BERT-2.0_SV/.venv_w2vbert_notebook/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: imports and device selection (minimal)\n",
    "import torch\n",
    "from importlib import reload\n",
    "\n",
    "# fresh-import useful during iterative development in notebook\n",
    "import w2vbert_speaker as eager_pkg\n",
    "import w2vbert_speaker_scripted as scripted_pkg\n",
    "reload(eager_pkg)\n",
    "reload(scripted_pkg)\n",
    "\n",
    "from w2vbert_speaker import W2VBERT_SPK_Module\n",
    "from w2vbert_speaker_scripted.runtime import W2VBERT_SPK_Scripted\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a461e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: /Users/zb/NWG/pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0/model_lmft_0.14.pth\n",
      "Using local encoder weights from: /Users/zb/NWG/pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0\n",
      "Preprocessed scripted artifact: /Users/zb/NWG/w2v-BERT-2.0_SV/packages/w2vbert_speaker/artifacts/w2vbert_speaker_script_preprocessed.pt\n",
      "Saved HF feature_extractor dir: /Users/zb/NWG/w2v-BERT-2.0_SV/packages/w2vbert_speaker/artifacts/feature_extractor\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: resolve model/checkpoint and artifact locations (adjust paths if needed)\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_checkpoint(repo_root: Path) -> Path:\n",
    "    candidate_relatives = [\n",
    "        'deeplab/pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0/model_lmft_0.14.pth',\n",
    "        '../pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0/model_lmft_0.14.pth',\n",
    "        'model_lmft_0.14.pth',\n",
    "    ]\n",
    "    for relative in candidate_relatives:\n",
    "        candidate = (repo_root / relative).resolve()\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError('Checkpoint model_lmft_0.14.pth was not found in known locations.')\n",
    "\n",
    "\n",
    "def resolve_model_dir(repo_root: Path) -> Path:\n",
    "    candidate_relatives = [\n",
    "        'deeplab/pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0',\n",
    "        '../pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0',\n",
    "    ]\n",
    "    for relative in candidate_relatives:\n",
    "        candidate = (repo_root / relative).resolve()\n",
    "        if candidate.is_dir() and (candidate / 'model.safetensors').exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError('Local Transformer weights not found. Expected model.safetensors under a known directory.')\n",
    "\n",
    "checkpoint_path = resolve_checkpoint(REPO_ROOT)\n",
    "model_dir = resolve_model_dir(REPO_ROOT)\n",
    "print(f'Using checkpoint: {checkpoint_path}')\n",
    "print(f'Using local encoder weights from: {model_dir}')\n",
    "\n",
    "# Paths to exported TorchScript artifacts and HF extractor saved by the exporter\n",
    "artifact_preprocessed = (REPO_ROOT / 'packages' / 'w2vbert_speaker' / 'artifacts' / 'w2vbert_speaker_script_preprocessed.pt').resolve()\n",
    "artifact_feature_extractor = (REPO_ROOT / 'packages' / 'w2vbert_speaker' / 'artifacts' / 'feature_extractor').resolve()\n",
    "print('Preprocessed scripted artifact:', artifact_preprocessed)\n",
    "print('Saved HF feature_extractor dir:', artifact_feature_extractor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c32cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu, target sample rate: 16000\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: load eager packaged model and determine target sample rate\n",
    "embedding_model = W2VBERT_SPK_Module(device=DEVICE, model_path=str(model_dir)).load_model(checkpoint_path)\n",
    "spk_model = embedding_model.modules_dict['spk_model']\n",
    "target_sr = spk_model.front.feature_extractor.sampling_rate\n",
    "print(f'Using device: {DEVICE}, target sample rate: {target_sr}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90910822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using audio file: /Users/zb/NWG/datasets/voxceleb1test/wav/id10270/5r0dWxy17C8/00001.wav\n",
      "Loaded waveform shape: torch.Size([1, 133761]), sample rate: 16000\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: load a small audio sample (adjust path to a local file if necessary)\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_audio_path(repo_root: Path) -> Path:\n",
    "    candidate_relatives = [\n",
    "        '../datasets/voxceleb1test/wav/id10270/5r0dWxy17C8/00001.wav',\n",
    "        'datasets/voxceleb1test/wav/id10270/5r0dWxy17C8/00001.wav',\n",
    "    ]\n",
    "    for relative in candidate_relatives:\n",
    "        candidate = (repo_root / relative).resolve()\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError('Audio file not found. Update resolve_audio_path with a valid sample.')\n",
    "\n",
    "\n",
    "def load_waveform(path: Path, target_sr: int) -> tuple[torch.Tensor, int]:\n",
    "    signal, sr = sf.read(str(path), dtype='float32')\n",
    "    if signal.ndim > 1:\n",
    "        signal = signal.mean(axis=1)\n",
    "    waveform = torch.from_numpy(signal)\n",
    "    if sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "        waveform = resampler(waveform.unsqueeze(0)).squeeze(0)\n",
    "        sr = target_sr\n",
    "    waveform = waveform.unsqueeze(0).to(torch.float32)\n",
    "    return waveform, sr\n",
    "\n",
    "AUDIO_PATH = resolve_audio_path(REPO_ROOT)\n",
    "waveform, sr = load_waveform(AUDIO_PATH, target_sr)\n",
    "print(f'Using audio file: {AUDIO_PATH}')\n",
    "print(f'Loaded waveform shape: {waveform.shape}, sample rate: {sr}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f690a4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager embedding shape: (256,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: run the eager packaged model to get an embedding (eager)\n",
    "import torch.nn.functional as F\n",
    "with torch.inference_mode():\n",
    "    embeddings = embedding_model(waveform.to(DEVICE))\n",
    "embedding_vector = embeddings.squeeze(0).detach().cpu().numpy()\n",
    "print(f'Eager embedding shape: {embedding_vector.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cfa2ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scripted (preprocessed) embedding shape: (256,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: load the scripted preprocessed artifact via the scripted wrapper and run it (safe)\n",
    "# Use the wrapper (not the underlying scripted module) so shapes are handled correctly.\n",
    "scripted_wrapper = W2VBERT_SPK_Scripted(scripted_path=str(artifact_preprocessed), feature_extractor_dir=str(artifact_feature_extractor), device=DEVICE)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # wrapper accepts waveform and will compute features, ensure we pass waveform with batch dim\n",
    "    emb_scripted = scripted_wrapper(waveform.cpu())\n",
    "    emb_scripted = emb_scripted.squeeze(0).detach().cpu()\n",
    "\n",
    "print(f'Scripted (preprocessed) embedding shape: {tuple(emb_scripted.shape)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea2aba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine(eager, scripted_preprocessed) = 1.000000\n",
      "    L2(eager, scripted_preprocessed) = 0.000000e+00\n",
      "Preprocessed TorchScript within tolerance of eager model.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: compute parity metrics between eager embedding and scripted embedding\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Convert eager to torch for cosine similarity\n",
    "import torch.nn.functional as F\n",
    "\n",
    "eager = torch.from_numpy(embedding_vector)\n",
    "scrip = emb_scripted\n",
    "cos_sim = F.cosine_similarity(eager.unsqueeze(0), scrip.unsqueeze(0)).item()\n",
    "l2 = float(norm(eager.numpy() - scrip.numpy()))\n",
    "print(f'cosine(eager, scripted_preprocessed) = {cos_sim:.6f}')\n",
    "print(f'    L2(eager, scripted_preprocessed) = {l2:.6e}')\n",
    "\n",
    "# Assert within a tight tolerance for the preprocessed artifact (expected near-exact parity)\n",
    "tolerance = 1e-5\n",
    "if l2 > tolerance:\n",
    "    raise AssertionError(f'Preprocessed TorchScript deviates from eager more than {tolerance}: L2={l2}')\n",
    "else:\n",
    "    print('Preprocessed TorchScript within tolerance of eager model.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "W2V-BERT Notebook (.venv_w2vbert_notebook)",
   "language": "python",
   "name": "w2vbert_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
