{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e85a2b",
   "metadata": {},
   "source": [
    "# W2V-BERT Embedding Demo (Packaged)\n",
    "\n",
    "This notebook demonstrates how to load the packaged (eager) W2V-BERT speaker model and run inference to produce embeddings. The repository has been standardized so notebooks use fixed, repo-relative artifact and dataset locations by default:\n",
    "\n",
    "- Eager artifacts: `../pretrained/w2vbert_speaker_eager`\n",
    "- Scripted artifacts: `../pretrained/w2vbert_speaker_scripted`\n",
    "- Dataset used in the examples: `../datasets/voxceleb1test`\n",
    "\n",
    "See `ARTIFACTS.md` at the repository root for what to download, expected files in each directory, and examples of how to override locations if you store artifacts elsewhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66c725e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /Users/zb/NWG/w2v-BERT-2.0_SV/recipes/DeepASV/notebooks\n",
      "Resolved repository root: /Users/zb/NWG/w2v-BERT-2.0_SV\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"recipes\").exists() and (candidate / \"deeplab\").exists():\n",
    "            return candidate\n",
    "    raise RuntimeError(f\"Unable to locate the repository root from {start}.\")\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "REPO_ROOT = find_repo_root(NOTEBOOK_DIR)\n",
    "\n",
    "print(f\"Notebook directory: {NOTEBOOK_DIR}\")\n",
    "print(f\"Resolved repository root: {REPO_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e74052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: /Users/zb/NWG/pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0/model_lmft_0.14.pth\n",
      "Using local encoder weights from: /Users/zb/NWG/pretrained/audio2vector/ckpts/facebook/w2v-bert-2.0\n",
      "Using device: cpu, target sample rate: 16000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from w2vbert_speaker import W2VBERT_SPK_Module\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use fixed, repo-relative locations for artifacts and data (per project convention)\n",
    "# EAGER artifacts directory contains eager packaged model & checkpoint\n",
    "EAGER_DIR = (REPO_ROOT.parent / \"pretrained\" / \"w2vbert_speaker_eager\").resolve()\n",
    "# SCRIPTED artifacts directory contains the exported TorchScript and feature-extractor\n",
    "SCRIPTED_DIR = (REPO_ROOT.parent / \"pretrained\" / \"w2vbert_speaker_scripted\").resolve()\n",
    "\n",
    "checkpoint_path = (EAGER_DIR / \"model_lmft_0.14.pth\").resolve()\n",
    "model_dir = EAGER_DIR\n",
    "\n",
    "print(f\"Using checkpoint: {checkpoint_path}\")\n",
    "print(f\"Using eager model dir: {model_dir}\")\n",
    "print(f\"Using scripted artifacts dir: {SCRIPTED_DIR}\")\n",
    "\n",
    "embedding_model = W2VBERT_SPK_Module(device=DEVICE, model_path=str(model_dir)).load_model(checkpoint_path)\n",
    "spk_model = embedding_model.modules_dict[\"spk_model\"]\n",
    "target_sr = spk_model.front.feature_extractor.sampling_rate\n",
    "print(f\"Using device: {DEVICE}, target sample rate: {target_sr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c7c0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using audio file: /Users/zb/NWG/datasets/voxceleb1test/wav/id10270/5r0dWxy17C8/00001.wav\n",
      "Loaded waveform shape: torch.Size([1, 133761]), sample rate: 16000\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "import torchaudio\n",
    "\n",
    "# Standardized dataset directory\n",
    "DATASET_DIR = (REPO_ROOT.parent / \"datasets\" / \"voxceleb1test\").resolve()\n",
    "\n",
    "def load_waveform(path: Path, target_sr: int) -> tuple[torch.Tensor, int]:\n",
    "    signal, sr = sf.read(str(path), dtype=\"float32\")\n",
    "    if signal.ndim > 1:\n",
    "        signal = signal.mean(axis=1)\n",
    "    waveform = torch.from_numpy(signal)\n",
    "    if sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "        waveform = resampler(waveform.unsqueeze(0)).squeeze(0)\n",
    "        sr = target_sr\n",
    "    waveform = waveform.unsqueeze(0).to(torch.float32)\n",
    "    return waveform, sr\n",
    "\n",
    "# Prefer a canonical example path under DATASET_DIR; update if you want a different sample\n",
    "AUDIO_PATH = (DATASET_DIR / \"wav\" / \"id10270\" / \"5r0dWxy17C8\" / \"00001.wav\").resolve()\n",
    "waveform, sr = load_waveform(AUDIO_PATH, target_sr)\n",
    "print(f\"Using audio file: {AUDIO_PATH}\")\n",
    "print(f\"Loaded waveform shape: {waveform.shape}, sample rate: {sr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf127f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (256,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.16015907, -0.8298738 ,  0.70560724, -0.14280552,  0.35778913,\n",
       "        0.47867072, -0.12521656, -0.90179497], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    embeddings = embedding_model(waveform.to(DEVICE))\n",
    "embedding_vector = embeddings.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(f\"Embedding shape: {embedding_vector.shape}\")\n",
    "embedding_vector[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c90e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "\n",
    "artifact_waveform = (SCRIPTED_DIR / \"w2vbert_speaker_script.pt\").resolve()\n",
    "artifact_preprocessed = (SCRIPTED_DIR / \"w2vbert_speaker_script_preprocessed.pt\").resolve()\n",
    "\n",
    "metadata_waveform = {\"sample_rate\": \"\", \"embedding_dim\": \"\"}\n",
    "scripted_waveform = torch.jit.load(str(artifact_waveform), map_location=\"cpu\", _extra_files=metadata_waveform)\n",
    "metadata_preprocessed = {\"sample_rate\": \"\", \"embedding_dim\": \"\", \"preprocessed\": \"\"}\n",
    "scripted_preprocessed = torch.jit.load(str(artifact_preprocessed), map_location=\"cpu\", _extra_files=metadata_preprocessed)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    waveform_scripted = waveform.cpu()\n",
    "    emb_waveform = scripted_waveform(waveform_scripted).squeeze(0).cpu()\n",
    "\n",
    "features = spk_model.front.feature_extractor(\n",
    "    waveform_scripted.squeeze(0).cpu().numpy(),\n",
    "    sampling_rate=target_sr,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=False,\n",
    "    truncation=False,\n",
    "    return_attention_mask=False,\n",
    ")\n",
    "input_features = features[\"input_features\"].to(torch.float32)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    emb_preprocessed = scripted_preprocessed(input_features).squeeze(0).cpu()\n",
    "\n",
    "cos_waveform = F.cosine_similarity(torch.from_numpy(embedding_vector).unsqueeze(0), emb_waveform.unsqueeze(0)).item()\n",
    "l2_waveform = torch.norm(torch.from_numpy(embedding_vector) - emb_waveform).item()\n",
    "cos_preprocessed = F.cosine_similarity(torch.from_numpy(embedding_vector).unsqueeze(0), emb_preprocessed.unsqueeze(0)).item()\n",
    "l2_preprocessed = torch.norm(torch.from_numpy(embedding_vector) - emb_preprocessed).item()\n",
    "\n",
    "print(f\"cosine(eager, waveform_script) = {cos_waveform:.6f}\")\n",
    "print(f\"    L2(eager, waveform_script) = {l2_waveform:.6e}\")\n",
    "print(f\"cosine(eager, preprocessed_script) = {cos_preprocessed:.6f}\")\n",
    "print(f\"    L2(eager, preprocessed_script) = {l2_preprocessed:.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1600a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-5\n",
    "if l2_preprocessed > tolerance:\n",
    "    raise AssertionError(f\"Preprocessed TorchScript deviates from eager more than {tolerance}: L2={l2_preprocessed}\")\n",
    "if l2_waveform > 1e-3:\n",
    "    print(f\"Warning: waveform TorchScript deviates from eager (L2={l2_waveform:.6e}). Use the preprocessed artifact for parity.\")\n",
    "else:\n",
    "    print(\"Waveform TorchScript within tolerance of eager model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c385194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-w2vbert-notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
